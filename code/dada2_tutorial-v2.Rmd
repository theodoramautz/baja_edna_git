---
title: "dada2_tutorial"
output: html_document
date: "2024-02-09"
---

```{r libraries}
library(tidyverse)
library(dada2)
library(digest)
library(seqinr)
library(here)
```

```{r files}
# Set the path to the directory where your trimmed sequences live. Remember, because you are working in an R Project the 'root' should be wherever you started the project.

path <- here("data", "baja_raw_sequences", "cutadapt-test")
list.files(path)
```

```{r filenames}
# Set 'pattern' to whatever your forward and reverse files end with; e.g., maybe the samples names are something like 'SAMPLENAME_R1_001.fastq' and 'SAMPLENAME_R2_001.fastq'
fnFs <- sort(list.files(path, pattern="_R1_001-trimmed.fastq", full.names = TRUE))
fnRs <- sort(list.files(path, pattern="_R2_001-trimmed.fastq", full.names = TRUE))

# Extract sample names, assuming filenames have format: SAMPLENAME-NUMBER_XXX.fastq
sample.names.1 <- sapply(strsplit(basename(fnFs), "_"), `[`, 1)
sample.names.2 <- sapply(strsplit(basename(fnFs), "_"), `[`, 2)
sample.names <- paste(sample.names.1, sample.names.2, sep="_")

# Check the sample names are as expected
sample.names
```

```{r read quality}
# Plot the read quality profiles of the first two samples
plotQualityProfile(fnFs[1:2])
```
```{r filter trim}
# Place filtered files in subdirectory
filtFs <- file.path(path, "dada2_filtered", paste0(sample.names, "_F_filt.fastq.gz"))
filtRs <- file.path(path, "dada2_filtered", paste0(sample.names, "_R_filt.fastq.gz"))
names(filtFs) <- sample.names
names(filtRs) <- sample.names

# Filter and trim sequences based on the truncation lengths you generated previously

### EXCEPTION FOR 12S MIFISH PRIMERS: Because so many amplified reads are bacterial, we have to preserve only reads shorter than the maximum allowed length of 110 bp (both forward and reverse). So for this data set, use 110 for both truncLen values if the calculated values are larger (which as we found they were!).

out <- filterAndTrim(fnFs, filtFs, fnRs, filtRs, 
                     truncLen=c(110,110), maxN=0, maxEE=c(2,2), 
                     truncQ=2, rm.phix=TRUE,
                     compress=TRUE, multithread=TRUE)
head(out)
```

```{r error rates}
# DADA2 uses a subset of samples to "learn" the sequencing error profiles
errF <- learnErrors(filtFs, multithread=TRUE)
errR <- learnErrors(filtRs, multithread=TRUE)
plotErrors(errF, nominalQ=TRUE)
```

```{r sample inference}
## If you get an error suggesting some of your samples had zero reads after filtering, you will have to filter out the empty samples. Here I am filtering out all samples with fewer than 20 reads, which is all three controls (two negative and one positive)
out_filt <- out %>% base::subset(out[,"reads.out"] > 20)
sample.names.filt.1 <- sapply(strsplit(rownames(out_filt), "-"), `[`, 1)
sample.names.filt.2 <- sapply(strsplit(rownames(out_filt), "-"), `[`, 2)
sample.names.filt <- paste(sample.names.filt.1, sample.names.filt.2, sep="-")

# Rename your filtered samples based on the new list of sample names
filtFs <- file.path(path, "dada2_filtered", paste0(sample.names.filt, "_F_filt.fastq.gz"))
filtRs <- file.path(path, "dada2_filtered", paste0(sample.names.filt, "_R_filt.fastq.gz"))

# Then repeat the dada commands above!
# This sample inference step uses the error profiles you generated above
dadaFs <- dada(filtFs, err=errF, multithread=TRUE)
dadaRs <- dada(filtRs, err=errR, multithread=TRUE)
```

```{r merge reads}
# Merge the paired-end reads
mergers <- mergePairs(dadaFs, filtFs, dadaRs, filtRs, verbose=TRUE)

# Inspect the merger data.frame from the first sample
head(mergers[[1]])
```

```{r asv table}
# Make the ASV table!
seqtab <- makeSequenceTable(mergers)
dim(seqtab)

# Inspect distribution of sequence lengths
table(nchar(getSequences(seqtab)))
```

```{r remove chimeras}
# Remove chimeras
seqtab.nochim <- removeBimeraDenovo(seqtab, method="consensus", multithread=TRUE, verbose=TRUE)
dim(seqtab.nochim)

sum(seqtab.nochim)/sum(seqtab)
```

```{r size filtering}
# Some primers (particularly 12S MiFish) amplify lots of off-target reads that may be shorter or longer than our target reads. This step filters out reads longer or shorter than our desired length; in this case MiFish lengths should be between 163 and 185 bp.

indexes.to.keep <- which((nchar(colnames(seqtab.nochim)) <= 185) & ((nchar(colnames(seqtab.nochim))) >= 163))
cleaned.seqtab.nochim <- seqtab.nochim[,indexes.to.keep]
```


```{r sanity check}
# Make a table to show how many reads were retained at each step of the process

getN <- function(x) sum(getUniques(x))
track <- cbind(out, sapply(dadaFs, getN), sapply(dadaRs, getN), sapply(mergers, getN), rowSums(cleaned.seqtab.nochim))
colnames(track) <- c("input", "filtered", "denoisedF", "denoisedR", "merged", "nonchim")
rownames(track) <- sample.names
head(track)
```

```{r assign taxonomy}
# Now we are ready to assign taxonomy to our ASVs. Change the database file name to include the path on your system.

taxa <- assignTaxonomy(cleaned.seqtab.nochim, "12S_efc_derep_and_clean-DADA2.fasta", tryRC=TRUE, multithread=TRUE)

# Removing sequence rownames for display only
taxa.print <- taxa 
rownames(taxa.print) <- NULL

head(taxa.print)
```

```{r save files}
# If you are satisfied with your results you can save the outputs

# First save the taxa table (no read data) and read provenance table
taxa.df <- as.data.frame(taxata)
write_csv(taxa.df, "NCOG_MiFish_taxa_table.csv")
track.df <- as.data.frame(track)
write_csv(track.df, "NCOG_MiFish_reads_track.csv")

# create hash key and link each ASV to a unique hash
seqtab.nochim.df <- as.data.frame(cleaned.seqtab.nochim)
conv_table <- tibble( Hash = "", Sequence ="")
Hashes <- map_chr (colnames(seqtab.nochim.df), ~ digest(.x, algo = "sha1", serialize = F, skip = "auto"))
conv_table <- tibble (Hash = Hashes,
                          Sequence = colnames(seqtab.nochim.df))
    
# write hash key into a csv
write_csv(conv_table, "NCOG_MiFish_hash_key.csv") 
# write hash key into a fasta
write.fasta(sequences = as.list(conv_table$Sequence), 
                names = as.list(conv_table$Hash),
                file.out = "NCOG_MiFish_ASVs.fasta")

# Make the ASV table
sample.df <- tibble::rownames_to_column(seqtab.nochim.df,"Sample_name")
sample.df <- data.frame(append(sample.df,c(Label="MiFish"), after = 1))
current_asv <- bind_cols(sample.df %>%
                           dplyr::select(Sample_name, Label),
                         seqtab.nochim.df)
current_asv <- current_asv %>%
  pivot_longer(cols = c(- Sample_name, - Label),
               names_to = "Sequence",
               values_to = "nReads") %>%
  filter(nReads > 0)
current_asv <- merge(current_asv,conv_table, by="Sequence") %>%
  select(-Sequence) %>%
  relocate(Hash, .after=Label)

# write asv table into a csv
write_csv(current_asv, "NCOG_MiFish_ASVs.csv") 
```