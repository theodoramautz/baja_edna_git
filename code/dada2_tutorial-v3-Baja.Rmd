---
title: "dada2_tutorial"
output: html_document
date: "2024-02-09"
---

```{r libraries}
library(tidyverse)
library(dada2)
library(digest)
library(seqinr)
library(here)
```

```{r files}
# Set the path to the directory where your trimmed sequences live. Remember, because you are working in an R Project the 'root' should be wherever you started the project.

path <- here("data", "baja_raw_sequences", "cutadapt-test")
list.files(path)
```

```{r filenames}
# Set 'pattern' to whatever your forward and reverse files end with; e.g., maybe the samples names are something like 'SAMPLENAME_R1_001.fastq' and 'SAMPLENAME_R2_001.fastq'
# Dada2 can use gzipped fastq files!
fnFs <- sort(list.files(path, pattern="_R1_001-trimmed.fastq", full.names = TRUE))
fnRs <- sort(list.files(path, pattern="_R2_001-trimmed.fastq", full.names = TRUE))

# Extract sample names, assuming filenames have format: SAMPLENAME-NUMBER_XXX.fastq
# sample.names.1 <- sapply(strsplit(basename(fnFs), "_"), `[`, 1)
# sample.names.2 <- sapply(strsplit(basename(fnFs), "_"), `[`, 2)
# sample.names <- paste(sample.names.1, sample.names.2, sep="_")

#Baja
sample.names <- sapply(strsplit(basename(fnFs), "_"), `[`, 2)

# Check the sample names are as expected
sample.names
```

```{r read quality}
# Plot the read quality profiles of the first two samples
plotQualityProfile(fnFs[1:2])
```

```{r filter trim}
# Place filtered files in subdirectory (does not need to exist beforehand, the command will create it)
filtFs <- file.path(path, "dada2_filtered", paste0(sample.names, "_F_filt.fastq.gz"))
filtRs <- file.path(path, "dada2_filtered", paste0(sample.names, "_R_filt.fastq.gz"))
names(filtFs) <- sample.names
names(filtRs) <- sample.names

# Filter and trim sequences based on the truncation lengths you generated previously

### EXCEPTION FOR 12S MIFISH PRIMERS: Because so many amplified reads are bacterial, we have to preserve only reads shorter than the maximum allowed length of 110 bp (both forward and reverse). So for this data set, use 110 for both truncLen values if the calculated values are larger (which as we found they were!).

out <- filterAndTrim(fnFs, filtFs, fnRs, filtRs, 
                     truncLen=c(110,110), maxN=0, maxEE=c(2,2), 
                     truncQ=2, rm.phix=TRUE,
                     compress=TRUE, multithread=TRUE)
head(out)
```
```{r filter empties}
# Check for samples that are empty (fewer than 10 sequences) following all the processing
out.df <- as.data.frame(out)
out.df.filt <- dplyr::filter(out.df, reads.out > 0)
sample.names <- sapply(strsplit(rownames(out.df.filt), "_"), `[`, 2)
filtFs.filt <- filtFs[sample.names]
filtRs.filt <- filtRs[sample.names]
out.filt <- out[rownames(out.df.filt), ]
```

```{r error rates}
# DADA2 uses a subset of samples to "learn" the sequencing error profiles
errF <- learnErrors(filtFs.filt, multithread=TRUE)
errR <- learnErrors(filtRs.filt, multithread=TRUE)
plotErrors(errF, nominalQ=TRUE)
```

```{r sample inference}
# This sample inference step uses the error profiles you generated above
dadaFs <- dada(filtFs.filt, err=errF, multithread=TRUE)
dadaRs <- dada(filtRs.filt, err=errR, multithread=TRUE)
```

```{r merge reads}
# Merge the paired-end reads
mergers <- mergePairs(dadaFs, filtFs.filt, dadaRs, filtRs.filt, verbose=TRUE)

# Inspect the merger data.frame from the first sample
head(mergers[[1]])
```

```{r asv table}
# Make the ASV table!
seqtab <- makeSequenceTable(mergers)
dim(seqtab)

# Inspect distribution of sequence lengths
table(nchar(getSequences(seqtab)))
```

```{r remove chimeras}
# Remove chimeras
seqtab.nochim <- removeBimeraDenovo(seqtab, method="consensus", multithread=TRUE, verbose=TRUE)
dim(seqtab.nochim)

sum(seqtab.nochim)/sum(seqtab)
```

```{r size filtering}
# Some primers (particularly 12S MiFish) amplify lots of off-target reads that may be shorter or longer than our target reads. This step filters out reads longer or shorter than our desired length; in this case MiFish lengths should be between 163 and 185 bp.

indexes.to.keep <- which((nchar(colnames(seqtab.nochim)) <= 185) & ((nchar(colnames(seqtab.nochim))) >= 163))
cleaned.seqtab.nochim <- seqtab.nochim[,indexes.to.keep]
```


```{r sanity check}
# Make a table to show how many reads were retained at each step of the process

getN <- function(x) sum(getUniques(x))
track <- cbind(out.filt, sapply(dadaFs, getN), sapply(dadaRs, getN), sapply(mergers, getN), rowSums(cleaned.seqtab.nochim))
colnames(track) <- c("input", "filtered", "denoisedF", "denoisedR", "merged", "nonchim")
rownames(track) <- sample.names
head(track)
```

```{r quality control}
# Check for samples that are empty (fewer than 10 sequences) following all the processing
track.df <- as.data.frame(track)
track.df.filt <- dplyr::filter(track.df, nonchim > 10)
sample.names.filt <- rownames(track.df.filt)
cleaned.seqtab.nochim.filt <- cleaned.seqtab.nochim[sample.names.filt,]
```

```{r assign taxonomy}
# Now we are ready to assign taxonomy to our ASVs. Change the database file name to include the path on your system.

taxa <- assignTaxonomy(cleaned.seqtab.nochim.filt, "/Users/theodoramautz/Desktop/SIO/Research/baja_edna/baja_edna_git/data/baja_raw_sequences/12S_efc_derep_and_clean-MURI-v2.fasta", tryRC=TRUE, multithread=TRUE)

# Removing sequence rownames for display only
taxa.print <- taxa 
rownames(taxa.print) <- NULL

head(taxa.print)
```

```{r save files}
# If you are satisfied with your results you can save the outputs

# First save the taxa table (no read data) and read provenance table
taxa.df <- as.data.frame(taxa)
write.csv(taxa.df, "Baja_MiFish_taxa_table.csv", row.names=TRUE)
track.df <- as.data.frame(track)
write.csv(track.df, "Baja_MiFish_reads_track.csv", row.names=TRUE)

# create hash key and link each ASV to a unique hash
seqtab.nochim.df <- as.data.frame(cleaned.seqtab.nochim.filt)
conv_table <- tibble( Hash = "", Sequence ="")
Hashes <- map_chr (colnames(seqtab.nochim.df), ~ digest(.x, algo = "sha1", serialize = F, skip = "auto"))
conv_table <- tibble (Hash = Hashes,
                          Sequence = colnames(seqtab.nochim.df))
    
# write hash key into a csv
write_csv(conv_table, "Baja_MiFish_hash_key.csv") 
# write hash key into a fasta
write.fasta(sequences = as.list(conv_table$Sequence), 
                names = as.list(conv_table$Hash),
                file.out = "Baja_MiFish_ASVs.fasta")

# Make the ASV table
sample.df <- tibble::rownames_to_column(seqtab.nochim.df,"Sample_name")
sample.df <- data.frame(append(sample.df,c(Label="MiFish"), after = 1))
current_asv <- bind_cols(sample.df %>%
                           dplyr::select(Sample_name, Label),
                         seqtab.nochim.df)
current_asv <- current_asv %>%
  pivot_longer(cols = c(- Sample_name, - Label),
               names_to = "Sequence",
               values_to = "nReads") %>%
  filter(nReads > 0)
current_asv <- merge(current_asv,conv_table, by="Sequence") %>%
  select(-Sequence) %>%
  relocate(Hash, .after=Label)

# write asv table into a csv
write_csv(current_asv, "Baja_MiFish_ASVs.csv") 
```

This completes the DADA2 part of the tutorial! Now we can use the resulting data (ASV counts and ASV taxonomy) to run some basic diversity analyses using a program called Phyloseq.

```{r phyloseq setup}
library(phyloseq); packageVersion("phyloseq")
library(Biostrings); packageVersion("Biostrings")
library(ggplot2); packageVersion("ggplot2")

# Set plot theme to a simple black-and-white design
theme_set(theme_bw())
```

```{r phyloseq preprocessing}
# Import the sample metadata file
metadata <- read.csv("Baja/Baja_metadata.csv")

# Set the metadata row names to the sample names (original full list). THIS ONLY WORKS IF YOUR SAMPLES ARE IN THE SAME ORDER IN BOTH TABLES! If they are not, re-order them as needed.
rownames(metadata) <- rownames(cleaned.seqtab.nochim)

# Remove any samples you excluded from the dada2 analysis
metadata.filt <- metadata[rownames(cleaned.seqtab.nochim.filt), ]

# Make the phyloseq object from teh ASV table, taxonomy table, and metadata table
ps <- phyloseq(otu_table(cleaned.seqtab.nochim.filt, taxa_are_rows=FALSE),
               sample_data(metadata.filt),
               tax_table(taxa))

# We can rename the ASV IDs to a short identifier instead of the full sequence
# The sequence will be stored in the refseq "slot" of the phyloseq object
dna <- Biostrings::DNAStringSet(taxa_names(ps))
names(dna) <- taxa_names(ps)
ps <- merge_phyloseq(ps, dna)
taxa_names(ps) <- paste0("ASV", seq(ntaxa(ps)))
ps

# Transform data to proportions as appropriate for Bray-Curtis distances
ps.prop <- transform_sample_counts(ps, function(otu) otu/sum(otu))

# The sample_data has sample names as rownames, but we can add a column with the names as well
sample_data(ps.prop)['sample.id'] <- row.names(sample_data(ps.prop))
```
Now we can run some basic diversity analyses on both data sets (with and without control samples)

```{r phyloseq alpha diversity}
# Plot alpha diversity metrics
plot_richness(ps.prop, x="Depth_group_m", measures=c("Shannon", "Simpson"), color="Control")
```

```{r phyloseq beta diversity}
# Plot beta diversity in NMDS

# Generate the ordinations using the Bray-Curtis distance metric
ord.nmds.bray <- ordinate(ps.prop, "NMDS", "bray")

# Plot!

# With controls
p1 <- plot_ordination(ps.prop, ord.nmds.bray, 
                      color="Depth_group_m", 
                      title="Bray NMDS",
                      label="sample.id")
p1


# The section below was written for NCOG data, adjust as needed for Baja.

# Looks like FISH_125 sample is a real outlier! Let's see if we can figure out why
# To explore the phyloseq we need to convert the object components to data frames
asv <- data.frame(otu_table(ps.prop.pruned))
tax <- data.frame(tax_table(ps.prop.pruned))
sd <- data.frame(sample_data(ps.prop.pruned))

# Using the "ASV" data frame let's look at FISH_125
asv["MFU-FISH_125", ]

# Most (>70%) of the ASVs belong to ASV208. What is the taxonomy of that ASV?
tax["ASV208",]

# all we know is that it's in the Phylum Chordata
# You can try running the sequence through NCBI BLAST

# Let's go back to our original "track" table and see how many ASVs ended up in the final count
head(track.df.filt)

# Only 29 "nonchim" reads in this sample, and >70% of them are one ASV! We can safely say this is a low-quality sample and we can discard it.

# Filter this sample and re-run the analyses as before. How do the alpha and beta diversity charts look now?
# See lines 212 and 213 for removing a sample from the data set

# Here is my final ordination plot (I called the new data set "ps.prop.pruned.v2")
# You can find more examples of ordination in phyloseq here: https://joey711.github.io/phyloseq/plot_ordination-examples.html

p2 <- plot_ordination(ps.prop.pruned.v2, ord.nmds.bray.pruned, 
                      color="Depth_group_m", 
                      shape="Season",
                      title="Bray NMDS") +
  theme_classic()
p2
```

```{r phyloseq bar plots}
top20 <- names(sort(taxa_sums(ps.prop.pruned.v2), decreasing=TRUE))[1:20]
#ps.top20 <- transform_sample_counts(ps, function(OTU) OTU/sum(OTU))
ps.top20 <- prune_taxa(top20, ps.prop.pruned.v2)
plot_bar(ps.top20, x="Depth_group_m", fill="Family") + facet_wrap(~Season, scales="free_x")

# Uh oh, looks like we have some non-fish families in here! Try to filter out these taxa and re-run the analyses. You may have to re-filter samples as well.
```